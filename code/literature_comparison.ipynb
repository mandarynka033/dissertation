{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub34xiTQfnLI"
      },
      "source": [
        "## Library import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "! pip install transformers\n",
        "! pip install --upgrade gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXIOY1lAf2Mk",
        "outputId": "beaad147-39af-49ab-969a-ffeb3b1099cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWaPvhe3fnLL"
      },
      "outputs": [],
      "source": [
        "# Language processing\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# System\n",
        "import sys\n",
        "path_smt = '/content/drive/MyDrive/dis/'\n",
        "sys.path.append(path_smt)\n",
        "\n",
        "# Data preprocessing\n",
        "from preprocessing import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Statistical tools\n",
        "import scipy.stats as stat\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec\n",
        "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "mrf40JcdiMIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lGSK0-3fnLP"
      },
      "outputs": [],
      "source": [
        "# Vector average\n",
        "def comp_aver(var,  j):\n",
        "    word_vectors = np.zeros((300, len(var[j])))\n",
        "    for i in range(len(var[j])):\n",
        "        wrd = var[j][i]\n",
        "        word_vectors[:, i] = google_news_vectors[wrd]\n",
        "#             Return the average of all word vectors\n",
        "    return(np.sum(word_vectors, axis = 1)/len(var[j]))\n",
        "\n",
        "# Vector average for the whole dataframe\n",
        "def add_stats(dt1, dt2):\n",
        "    sims = np.zeros(len(dt1))\n",
        "    for jj in range(len(dt1)):\n",
        "        avg_vec1 = comp_aver(dt1, j = jj)\n",
        "        avg_vec2 = comp_aver(dt2, j = jj)\n",
        "\n",
        "#         Compute cosine distance\n",
        "        sims[jj] = 1 - spatial.distance.cosine(avg_vec1, avg_vec2)\n",
        "\n",
        "    return(sims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp7hM1xefnLO"
      },
      "outputs": [],
      "source": [
        "# Compute correlation for a dataset\n",
        "def subsets(dt):\n",
        "  # Create a dataframe with the whole vocabulary\n",
        "  dt_long = pd.concat([dt[[\"sent1\"]].rename(columns = {\"sent1\":\"sent\"}), \n",
        "                     dt[[\"sent2\"]].rename(columns={\"sent2\":\"sent\"})], ignore_index=True)\n",
        "  # Preprocess\n",
        "  preprocess(\"sent\", dt_long)\n",
        "  # Creating the vocabulary collection\n",
        "  all_words = set(' '.join(dt_long['sent_punct']).split())   \n",
        "  # Subsetting all words that are not present\n",
        "  add_google = all_words.difference(google_lst)\n",
        "  # Add OOV words to the dictionary\n",
        "  for wrd in add_google:\n",
        "    google_news_vectors[wrd] = np.random.rand(300)\n",
        "\n",
        "  # Add the preprocessed words to the original dataframe\n",
        "  dt.loc[:,(\"sent_punct1\")] = dt_long.sent_punct[0:int((len(dt_long)/2))]\n",
        "  dt.loc[:,(\"sent_punct2\")] = dt_long[int((len(dt_long)/2)):].reset_index().sent_punct\n",
        "  # Tokenising as the last preprcessing step\n",
        "  sent1 = dt.apply(lambda row: nltk.word_tokenize(row[\"sent_punct1\"]), axis=1)\n",
        "  sent2 = dt.apply(lambda row: nltk.word_tokenize(row[\"sent_punct2\"]), axis=1)\n",
        "\n",
        "  # Vector average\n",
        "  dt[\"aver\"] = add_stats(sent1, sent2)\n",
        "\n",
        "  # WMD\n",
        "  sims = np.zeros(len(dt))\n",
        "  for i in range(len(dt)):\n",
        "    sims[i] = wmdist(google_news_vectors, sent1[i], sent2[i])\n",
        "  # Adding the inverted distance to dataframe\n",
        "  dt[\"wmd\"] = 1/(1+sims)\n",
        "  # Correlation for vector average\n",
        "  cor_aver = stat.pearsonr(dt[\"aver\"], dt[\"sim\"])[0]\n",
        "  # Correlation for WMD\n",
        "  cor_wmd = stat.pearsonr(dt[\"wmd\"], dt[\"sim\"])[0]\n",
        "  return cor_aver, cor_wmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lepgBbLfnLN"
      },
      "source": [
        "## Data import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "par = pd.read_csv(path_smt+\"MSRpar.test.tsv\", sep='\\t', encoding='utf-8', header = None, \n",
        "                 names = ['sim', 'sent1', 'sent2'])\n",
        "vid = pd.read_csv(path_smt+\"MSRvid.test.tsv\", sep='\\t', encoding='utf-8')[[\"sent1\", \"sent2\", \"sim\"]]\n",
        "europarl = pd.read_csv(path_smt+\"SMTeuroparl.test.tsv\", sep='\\t', encoding='utf-8', header = None, \n",
        "                 names = ['sim', 'sent1', 'sent2'])\n",
        "wn = pd.read_csv(path_smt+\"OnWN.test.tsv\", sep='\\t', encoding='utf-8', header = None, \n",
        "                 names = ['sim', 'sent1', 'sent2'])\n",
        "news = pd.read_csv(path_smt+\"SMTnews.test.tsv\", sep='\\t', encoding='utf-8', header = None, \n",
        "                 names = ['sim', 'sent1', 'sent2'])"
      ],
      "metadata": {
        "id": "80-QR4DGlYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation for each dataset separately"
      ],
      "metadata": {
        "id": "2FWl36s5mbH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec keys from the dictionary\n",
        "google_lst = google_news_vectors.index_to_key"
      ],
      "metadata": {
        "id": "JjeSFC12h9d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-jLs2XrfnLW",
        "outputId": "92d66d19-11af-4bd8-cf78-fd856a666156"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16495297174562207, 0.4214197308002308)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "subsets(par)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsets(vid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1ltOkdvtOha",
        "outputId": "47840b56-b933-439c-fb53-77f199280ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7560301689154783, 0.6667328176199839)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsets(europarl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MelMR_T1NuY",
        "outputId": "f010f627-af1e-4ca6-8947-ad12425da9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.49591104375513795, 0.4946393836183523)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsets(wn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57A29vyy2FNf",
        "outputId": "c839bd8c-2b74-430b-ce62-e99405b18501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6376490071848622, 0.6679255783521272)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsets(news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPEO8gXF2TeG",
        "outputId": "f58d3fc6-d017-41e7-e649-89704371c22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3906771958717344, 0.45135647464089557)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction for all datasets from 2012"
      ],
      "metadata": {
        "id": "MFk4M4O28__x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sts2012 = pd.concat([par, vid, europarl, wn, news])"
      ],
      "metadata": {
        "id": "wj9vtZlQ4HAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Correlation for 2012 average:\", stat.pearsonr(sts2012[\"aver\"], sts2012[\"sim\"])[0])\n",
        "print(\"Correlation for 2012 wmd:\", stat.pearsonr(sts2012[\"wmd\"], sts2012[\"sim\"])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgBVqjpd8h2k",
        "outputId": "73ea2136-1d67-4cdb-a8ff-6e9ae13fe306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation for 2012 average: 0.5754661193605161\n",
            "Correlation for 2012 wmd: 0.5502980576648839\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "literature_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}